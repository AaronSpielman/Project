---
title: "Exploratory Plots and Clustering of NASA GISTEMP Data"
author: "Saurav Kiri"
date: '`r format(Sys.time(), "Last modified: %d %b %Y")`'
output:
  bookdown::gitbook:
    self_contained: true
    split_by: none
    sharing: null
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(tidyverse)
library(data.table)
library(corrplot)       # Make correlation plots
library(ComplexHeatmap) # For making heatmaps
library(ggfortify)      # For using autoplot() with prcomp()
library(ClusterR)       # For k-means clustering
library(factoextra)     # For elbow plots to determine optimal k
```

# Setup

A `setup.RData` file is available within the "Reports" folder of the GitHub repository. Here, the workspace image after pre-processing is loaded to facilitate the construction of plots.

```{r}
load("setup.RData")
```

## Heatmaps and Correlation Plots
We first wrangle the data to combine all temperature difference data to look at how related data from each of AIRSv6, AIRSv7, and GHCNv4 are:
```{r}
# What reduce() does is apply a binary function .f that has a singl return value
# To join elements of a vector/list into a single value/object
# E.g., to reduce a vector 1:3, reduce() performs f(f(1,2), 3)
temp.combin <- lapply(temp.data.converted.long, select_fun) %>% purrr::reduce(left_join, by = c("Year", "Month"))
colnames(temp.combin) <- c("Year", "Month", names(temp.data))
temp.combin %>%
    select_if(is.numeric) %>%
    select(!Year) %>%
    cor(use = "pairwise.complete.obs") %>%
    corrplot(method = "shade",
             type = "lower",
             diag = FALSE,
             addCoef.col = "white",
             tl.col = "black",
             order = "hclust")
```

The data are quite highly correlated. Therefore, any patterns found in one dataset will hopefully be recovered with similar accuracy across other datasets. In order to reduce the complexity of the next step of looking at correlation/heatmap analysis between years, I will simply use the GHCN v4 data, since it possesses complete data for 2002.

Heatmap production:
```{r}
temp.data.monthly <- lapply(temp.data.converted, function(df) df %>% select(contains(c(month.abb))))
for (i in 1:length(temp.data.monthly)) {
    rownames(temp.data.monthly[[i]]) <- temp.data.converted[[1]]$"Year"
}

# Use scale on the *columns* of temp.data.monthly since this data is in wide format
# I.e., for each month, we want to get a z-score of each year to determine level of extremeness
    # Want z-score of each year by month, not each month by year
# Use t() to get the df such that each column is a year, and each entry is that year
# As a z-score for the month
t(apply(temp.data.monthly[[3]], 2, scale)) %>%
    Heatmap(cluster_columns = TRUE,
            cluster_rows = FALSE,
            row_labels = month.abb,
            column_labels = 2002:2022,
            name = "Z-score")
```

Now, it would be interesting to check how the different years correlate in terms of their monthly average temperature anomalies. We employ the use of `corrplot` to do so:

```{r, fig.width = 10, fig.height = 10, fig.fullwidth = TRUE}
# Transpose to get correlation of years against each other, not months against each other
temp.data.monthly[["ghcnv4"]] %>%
  t() %>%
  cor(use = "pairwise.complete.obs") %>%
  corrplot(method = "shade",
           order = "AOE",
           addCoef.col = "white",
           tl.col = "black")
```


## PCA and K-Means Clustering
While we now have a better picture of the relative standings of each year with respect to one another, we wish now to explore how these years resolve into distinct groups. Therefore, we will employ PCA to be able to explore as much information of each year as possible without needing to explore $R^{12}$ space.

First, we use k-means clustering to obtain clusters of years. To test the optimal value of k, we produce an elbow plot to determine at which point the explained variance does not appreciably increase - to do this, we use the "within sum-of-squares" approach, in which we calculate the squared difference between each observation and its cluster mean to try to optimize capturing the maximum amount of variance without overfitting:

\[\sum_{k=1}^{K} \sum_{i \in S_k} (X_i - \overline{X}_k)^2 \]

Where $S_k$ is the set of all observations in the $k$-th cluster, $X_i$ is the $i$-th observation of this cluster, and $\overline{X}_k$ is the mean of the $k$-th cluster.

Elbow plot using `fviz_nbclust` from the `factoextra` package:
```{r}
# Keep only temperature anomaly data
# Remove 2022 since it has NA values, does not work with k-means
temp.ghcn <- temp.data.monthly[["ghcnv4"]] %>%
  select(Jan:Dec) %>%
  na.omit()

fviz_nbclust(temp.ghcn, kmeans, method = "wss")
```

From the above plot, it appears that roughly $k = 3$ is likely the most optimal choice for $k$ (though the biggest "bend" occurs at $k = 2$), since beyond $k = 3$, there is no significant increase in variance explained.

Therefore, we perform $k$-means cluster with 3 clusters using the `ClusterR` package:

```{r}
set.seed(1)
kmeans.temp <- kmeans(temp.ghcn, centers = 3, nstart = 20)
kmeans.temp
kmeans.temp$cluster
```

Finally, to investigate how well our so-identified clusters resolve, we use PCA and color the plot based on the unsupervised tagging from our clustering algorithm:

```{r}
# Use scale. = TRUE to scale variables to have unit variance
temp.ghcn.clustered <- cbind(temp.ghcn, Group = kmeans.temp$cluster)
temp.ghcn %>%
  prcomp(scale. = TRUE) %>%
  autoplot(loadings = TRUE,
           colour = temp.ghcn.clustered$Group,
           loadings.colour = "blue",
           loadings.label.colour = "magenta",
           loadings.label.repel = TRUE) +
           geom_text(vjust = -1,
                       label = rownames(temp.ghcn.clustered),
                       color = temp.ghcn.clustered$Group) +
           theme(axis.line = element_line(size = 1),
                 axis.title = element_text(size = 14),
                 axis.text = element_text(size = 14))
```